{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbfEtO7Y1YTg"
      },
      "outputs": [],
      "source": [
        "# https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification\n",
        "\n",
        "# https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEmaa2YAtskt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r /content/requirements.txt ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LwALv2SqHlZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "RM79aUWgpwox",
        "outputId": "9d685e27-ed13-4f93-ab29-62cc1e718514"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ab9d8b0a-e809-4cc5-b193-199db6b6e03c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>majority_label</th>\n",
              "      <th>source</th>\n",
              "      <th>annotator1</th>\n",
              "      <th>annotator2</th>\n",
              "      <th>annotator3</th>\n",
              "      <th>translated_texts</th>\n",
              "      <th>claim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ഇച്ഛാശക്തിയുള്ള ഒരു ഭരണകൂടത്തിന് അസാധ്യമായൊന്ന...</td>\n",
              "      <td>ml</td>\n",
              "      <td>Yes</td>\n",
              "      <td>social media</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>There is nothing impossible for a government t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>.            *_SKY ViSiON_*        / /        ...</td>\n",
              "      <td>ml</td>\n",
              "      <td>Yes</td>\n",
              "      <td>social media</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>== sync, corrected by elderman == @elder_man =...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>+------+------+-----+------+-----+-----+------...</td>\n",
              "      <td>ml</td>\n",
              "      <td>Yes</td>\n",
              "      <td>social media</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>The-------------------------------------------...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>ബിജെപി അല്ലെങ്കിൽ യുപിയിലെ സഖ്യം... ഇവരിൽ നിന്...</td>\n",
              "      <td>ml</td>\n",
              "      <td>Yes</td>\n",
              "      <td>social media</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Bijepi or the party in Jupiter... any of them ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>.            *_SKY ViSiON_*        / /        ...</td>\n",
              "      <td>ml</td>\n",
              "      <td>Yes</td>\n",
              "      <td>social media</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>*_SKY VISION_* / /_size_**_bulance_*_bulance_*...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab9d8b0a-e809-4cc5-b193-199db6b6e03c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab9d8b0a-e809-4cc5-b193-199db6b6e03c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab9d8b0a-e809-4cc5-b193-199db6b6e03c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  ... claim\n",
              "0           0  ...     1\n",
              "1           1  ...     1\n",
              "2           2  ...     1\n",
              "3           4  ...     1\n",
              "4           5  ...     1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data = pd.read_csv(\"all_translated_data.csv\", encoding = \"utf-8\")\n",
        "data[\"claim\"] = data[\"majority_label\"].apply(lambda x: 1 if x == \"Yes\" else (0 if x == \"No\" else \"No_Majority\"))\n",
        "# Just use the texts with a majority label\n",
        "data = data[data.claim != \"No_Majority\"]\n",
        "data = data.reset_index(drop=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BubLMIGVp6t3"
      },
      "outputs": [],
      "source": [
        "# Select all languages\n",
        "data_en= data.loc[(data.language == \"en\")]\n",
        "data_hi = data.loc[(data.language == \"hi\")]\n",
        "data_bn = data.loc[(data.language == \"bn\")]\n",
        "data_ta = data.loc[(data.language == \"ta\")]\n",
        "data_ml = data.loc[(data.language == \"ml\")]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this to make a bigger Dataframe with the selected language and the translated texts in the same language\n",
        "\n",
        "text_hi = data_hi[\"text\"].tolist()\n",
        "text_trans_hi = data_hi[\"translated_texts\"].tolist()\n",
        "claim_hi = data_hi[\"claim\"].tolist()\n",
        "claim_trans_hi = data_hi[\"claim\"].tolist()\n",
        "text_list_hi = text_hi + text_trans_hi\n",
        "claim_list_hi = claim_hi + claim_trans_hi\n",
        "df_dict_hi = {\"text\": text_list_hi, \"claim\": claim_list_hi}\n",
        "df_dict_hi\n",
        "hi_trans_df = pd.DataFrame(df_dict_hi)\n",
        "\n",
        "text_ta = data_ta[\"text\"].tolist()\n",
        "text_trans_ta = data_ta[\"translated_texts\"].tolist()\n",
        "claim_ta = data_ta[\"claim\"].tolist()\n",
        "claim_trans_ta = data_ta[\"claim\"].tolist()\n",
        "text_list_ta = text_ta + text_trans_ta\n",
        "claim_list_ta = claim_ta + claim_trans_ta\n",
        "df_dict_ta = {\"text\": text_list_ta, \"claim\": claim_list_ta}\n",
        "df_dict_ta\n",
        "ta_trans_df = pd.DataFrame(df_dict_ta)\n",
        "\n",
        "text_ml = data_ml[\"text\"].tolist()\n",
        "text_trans_ml = data_ml[\"translated_texts\"].tolist()\n",
        "claim_ml = data_ml[\"claim\"].tolist()\n",
        "claim_trans_ml = data_ml[\"claim\"].tolist()\n",
        "text_list_ml = text_ml + text_trans_ml\n",
        "claim_list_ml = claim_ml + claim_trans_ml\n",
        "df_dict_ml = {\"text\": text_list_ml, \"claim\": claim_list_ml}\n",
        "df_dict_ml\n",
        "ml_trans_df = pd.DataFrame(df_dict_ml)\n",
        "\n",
        "text_bn = data_bn[\"text\"].tolist()\n",
        "text_trans_bn = data_bn[\"translated_texts\"].tolist()\n",
        "claim_bn = data_bn[\"claim\"].tolist()\n",
        "claim_trans_bn = data_bn[\"claim\"].tolist()\n",
        "text_list_bn = text_bn + text_trans_bn\n",
        "claim_list_bn = claim_bn + claim_trans_bn\n",
        "df_dict_bn = {\"text\": text_list_bn, \"claim\": claim_list_bn}\n",
        "df_dict_bn\n",
        "bn_trans_df = pd.DataFrame(df_dict_bn)"
      ],
      "metadata": {
        "id": "ceZyhfeQTvK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1LjoPxsQYyI7",
        "outputId": "182a728c-2dc2-4f06-f238-89237074ab18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6a59c41e-f28f-47ec-95ed-0c6104c99550\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>majority_label</th>\n",
              "      <th>source</th>\n",
              "      <th>annotator1</th>\n",
              "      <th>annotator2</th>\n",
              "      <th>annotator3</th>\n",
              "      <th>translated_texts</th>\n",
              "      <th>claim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3145</th>\n",
              "      <td>4066</td>\n",
              "      <td>*कुसुम किसलय कुञ्ज कोकिल,*\\n*कूकते है फ़ाग में।...</td>\n",
              "      <td>hi</td>\n",
              "      <td>No</td>\n",
              "      <td>social media</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>* Kuzum Kway Kwanj Kokil, * * Kukukuk in Fagh....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3146</th>\n",
              "      <td>4067</td>\n",
              "      <td>जिसने जेल में 'कसाब' कोे 'कबाब' दिया और 'साध्व...</td>\n",
              "      <td>hi</td>\n",
              "      <td>No</td>\n",
              "      <td>social media</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Who gave 'Casab' to 'Casab' in prison and 'Saa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3147</th>\n",
              "      <td>4068</td>\n",
              "      <td>मेरी गली में एक आवारा कुत्ता है। बेबजह भौंकता ...</td>\n",
              "      <td>hi</td>\n",
              "      <td>No</td>\n",
              "      <td>social media</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>There's an air dog in my neck. I don't know.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3148</th>\n",
              "      <td>4069</td>\n",
              "      <td>\\nमौसम_अलर्ट:-\\n\\nसक्रिय प०वि० के प्रभाव से पा...</td>\n",
              "      <td>hi</td>\n",
              "      <td>Yes</td>\n",
              "      <td>social media</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Moussam_Alert:- Active dog breeders are becomi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3149</th>\n",
              "      <td>4070</td>\n",
              "      <td>शाहीन बाग में प्रदर्शन के दौरान महिलाओं के बीच...</td>\n",
              "      <td>hi</td>\n",
              "      <td>Yes</td>\n",
              "      <td>fact-check</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Marpet between women during the show in Shaun'...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4012</th>\n",
              "      <td>5061</td>\n",
              "      <td>सर हम लोगो को उत्तर प्रदेश बिजली विभाग ने तीन ...</td>\n",
              "      <td>hi</td>\n",
              "      <td>No</td>\n",
              "      <td>social media</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Sir, we have brought out the people from the w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4013</th>\n",
              "      <td>5062</td>\n",
              "      <td>_*आज के श्रृंगार दर्शन श्री गणेश जी के खजराना ...</td>\n",
              "      <td>hi</td>\n",
              "      <td>No</td>\n",
              "      <td>social media</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Probably</td>\n",
              "      <td>_*Today's fair exhibition Mr. G. G.'s treasure...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4014</th>\n",
              "      <td>5063</td>\n",
              "      <td>Fact Check: लोकसभा चुनावों में वोट नहीं देने प...</td>\n",
              "      <td>hi</td>\n",
              "      <td>Yes</td>\n",
              "      <td>fact-check</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Probably</td>\n",
              "      <td>Fact Check: If you don't vote in public electi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4015</th>\n",
              "      <td>5064</td>\n",
              "      <td>Fact Check: फारूक अब्दुल्ला के “भारत माता की ज...</td>\n",
              "      <td>hi</td>\n",
              "      <td>Yes</td>\n",
              "      <td>fact-check</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Fact Check: Faruk Abdulla's home is not after ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4016</th>\n",
              "      <td>5065</td>\n",
              "      <td>क्या यह योगी ३०० साल पहले समाधी लेने बाद अब तक...</td>\n",
              "      <td>hi</td>\n",
              "      <td>No</td>\n",
              "      <td>fact-check</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Is it still alive 300 years ago?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>872 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a59c41e-f28f-47ec-95ed-0c6104c99550')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6a59c41e-f28f-47ec-95ed-0c6104c99550 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6a59c41e-f28f-47ec-95ed-0c6104c99550');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0  ... claim\n",
              "3145        4066  ...     0\n",
              "3146        4067  ...     0\n",
              "3147        4068  ...     0\n",
              "3148        4069  ...     1\n",
              "3149        4070  ...     1\n",
              "...          ...  ...   ...\n",
              "4012        5061  ...     0\n",
              "4013        5062  ...     0\n",
              "4014        5063  ...     1\n",
              "4015        5064  ...     1\n",
              "4016        5065  ...     0\n",
              "\n",
              "[872 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsW1v8rovCfy"
      },
      "outputs": [],
      "source": [
        "# Create Train, Test, Validate for every language\n",
        "#data_en = data_en[[\"text\", \"claim\"]]\n",
        "data_hi = hi_trans_df[[\"text\", \"claim\"]]\n",
        "data_ml = ml_trans_df[[\"text\", \"claim\"]]\n",
        "data_ta = ta_trans_df[[\"text\", \"claim\"]]\n",
        "data_bn = bn_trans_df[[\"text\", \"claim\"]]\n",
        "\n",
        "data_en = data_en.rename({'text': 'sentence1', 'claim': 'label'}, axis='columns')\n",
        "data_hi = data_hi.rename({'text': 'sentence1', 'claim': 'label'}, axis='columns')\n",
        "data_ml = data_ml.rename({'text': 'sentence1', 'claim': 'label'}, axis='columns')\n",
        "data_ta = data_ta.rename({'text': 'sentence1', 'claim': 'label'}, axis='columns')\n",
        "data_bn = data_bn.rename({'text': 'sentence1', 'claim': 'label'}, axis='columns')\n",
        "\n",
        "#set seed\n",
        "np.random.seed(0)\n",
        "train_en, validate_en, test_en = np.split(data_en.sample(frac=1), [int(.6*len(data_en)), int(.8*len(data_en))])\n",
        "train_en.to_csv(\"train_en.csv\")\n",
        "validate_en.to_csv(\"validate_en.csv\")\n",
        "test_en.to_csv(\"test_en.csv\")\n",
        "\n",
        "#set seed\n",
        "np.random.seed(0)\n",
        "train_hi, validate_hi, test_hi = np.split(data_hi.sample(frac=1), [int(.6*len(data_hi)), int(.8*len(data_hi))])\n",
        "train_hi.to_csv(\"train_hi.csv\")\n",
        "validate_hi.to_csv(\"validate_hi.csv\")\n",
        "test_hi.to_csv(\"test_hi.csv\")\n",
        "\n",
        "#set seed\n",
        "np.random.seed(0)\n",
        "train_ml, validate_ml, test_ml = np.split(data_ml.sample(frac=1), [int(.6*len(data_ml)), int(.8*len(data_ml))])\n",
        "train_ml.to_csv(\"train_ml.csv\")\n",
        "validate_ml.to_csv(\"validate_ml.csv\")\n",
        "test_ml.to_csv(\"test_ml.csv\")\n",
        "\n",
        "#set seed\n",
        "np.random.seed(0)\n",
        "train_ta, validate_ta, test_ta = np.split(data_ta.sample(frac=1), [int(.6*len(data_ta)), int(.8*len(data_ta))])\n",
        "train_ta.to_csv(\"train_ta.csv\")\n",
        "validate_ta.to_csv(\"validate_ta.csv\")\n",
        "test_ta.to_csv(\"test_ta.csv\")\n",
        "\n",
        "#set seed\n",
        "np.random.seed(0)\n",
        "train_bn, validate_bn, test_bn = np.split(data_bn.sample(frac=1), [int(.6*len(data_bn)), int(.8*len(data_bn))])\n",
        "train_bn.to_csv(\"train_bn.csv\")\n",
        "validate_bn.to_csv(\"validate_bn.csv\")\n",
        "test_bn.to_csv(\"test_bn.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9CdNND5u4u5",
        "outputId": "af9b721c-8aa6-41bc-dcef-2c9733e60126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1164\n",
            "388\n",
            "388\n"
          ]
        }
      ],
      "source": [
        "print(len(train_ml))\n",
        "print(len(test_ml))\n",
        "print(len(validate_ml))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cf4fCk6jzh6E",
        "outputId": "d7def78a-5c54-4eb9-de88-247a44e7c015"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e8602b92-0c41-491e-a87a-be943b63f7c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>800</td>\n",
              "      <td>ബിജെപി ബിഡിജെഎസിനെ ചതിച്ചു...തുഷാറിന്‍റെ പ്രചാ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>281</td>\n",
              "      <td>നോമ്പ് മുസൽമാന്‌ നിർബന്ധം ഉള്ള കാര്യമാ എന്നാൽ ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>932</td>\n",
              "      <td>*തിരുവനന്തപുരം നഗരത്തിലെ എല്ലാ സര്‍ക്കാര്‍ സ്ഥ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>732</td>\n",
              "      <td>*(കേരളത്തിൽ രാഹുൽ ഗാന്ധി മത്സരിക്കുന്നതിനെ കുറ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>715</td>\n",
              "      <td>.            *_SKY VISION_*        / /        ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8602b92-0c41-491e-a87a-be943b63f7c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8602b92-0c41-491e-a87a-be943b63f7c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8602b92-0c41-491e-a87a-be943b63f7c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0                                          sentence1  label\n",
              "0         800  ബിജെപി ബിഡിജെഎസിനെ ചതിച്ചു...തുഷാറിന്‍റെ പ്രചാ...      0\n",
              "1         281  നോമ്പ് മുസൽമാന്‌ നിർബന്ധം ഉള്ള കാര്യമാ എന്നാൽ ...      1\n",
              "2         932  *തിരുവനന്തപുരം നഗരത്തിലെ എല്ലാ സര്‍ക്കാര്‍ സ്ഥ...      1\n",
              "3         732  *(കേരളത്തിൽ രാഹുൽ ഗാന്ധി മത്സരിക്കുന്നതിനെ കുറ...      1\n",
              "4         715  .            *_SKY VISION_*        / /        ...      1"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "test_ml[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-91mUU_VqM2G",
        "outputId": "eb9d27b5-3fb2-451b-f551-127d534887ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/20/2022 09:14:55 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gcp_project=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_en/runs/Jan20_09-14-55_56191b56896d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=output_en/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "poly_power=1.0,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_en/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_name=None,\n",
            "tpu_num_cores=None,\n",
            "tpu_zone=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xla=False,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/20/2022 09:14:55 - INFO - __main__ - Loading a local file for train: train_en.csv\n",
            "01/20/2022 09:14:55 - INFO - __main__ - Loading a local file for validation: validate_en.csv\n",
            "01/20/2022 09:14:55 - INFO - __main__ - Loading a local file for test: test_en.csv\n",
            "01/20/2022 09:14:55 - WARNING - datasets.builder - Using custom data configuration default-8a2f43fb9af59b8a\n",
            "01/20/2022 09:14:55 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-8a2f43fb9af59b8a/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
            "\r  0% 0/3 [00:00<?, ?it/s]\r100% 3/3 [00:00<00:00, 380.62it/s]\n",
            "Downloading: 100% 411/411 [00:00<00:00, 789kB/s]\n",
            "Downloading: 100% 181/181 [00:00<00:00, 263kB/s]\n",
            "Downloading: 100% 3.02M/3.02M [00:00<00:00, 42.7MB/s]\n",
            "Downloading: 100% 113/113 [00:00<00:00, 208kB/s]\n",
            "100% 1/1 [00:00<00:00, 17.59ba/s]\n",
            "100% 1/1 [00:00<00:00, 48.07ba/s]\n",
            "100% 1/1 [00:00<00:00, 50.24ba/s]\n",
            "2022-01-20 09:15:00.987829: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Downloading: 100% 1.45G/1.45G [01:22<00:00, 18.9MB/s]\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier', 'bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-01-20 09:16:26.689452: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 553\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:0\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Epoch 1/3\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.6522 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+002022-01-20 09:17:06.159976: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 185\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:4\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "2022-01-20 09:17:10.489314: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "69/69 [==============================] - 48s 370ms/step - loss: 0.6458 - accuracy: 0.6522 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5333 - val_accuracy: 0.8378 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/3\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.7210 - f1_m: 0.2039 - precision_m: 0.2282 - recall_m: 0.22292022-01-20 09:17:33.664515: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "69/69 [==============================] - 23s 330ms/step - loss: 0.5956 - accuracy: 0.7210 - f1_m: 0.2039 - precision_m: 0.2282 - recall_m: 0.2229 - val_loss: 0.6400 - val_accuracy: 0.6595 - val_f1_m: 0.0104 - val_precision_m: 0.0417 - val_recall_m: 0.0060\n",
            "Epoch 3/3\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.6690 - accuracy: 0.6630 - f1_m: 0.0730 - precision_m: 0.0940 - recall_m: 0.06842022-01-20 09:17:56.833261: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "69/69 [==============================] - 23s 335ms/step - loss: 0.6690 - accuracy: 0.6630 - f1_m: 0.0730 - precision_m: 0.0940 - recall_m: 0.0684 - val_loss: 0.6297 - val_accuracy: 0.6270 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "01/20/2022 09:18:00 - INFO - __main__ - Doing predictions on test dataset...\n",
            "2022-01-20 09:18:00.709184: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 185\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:7\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "01/20/2022 09:18:04 - INFO - __main__ - Wrote predictions to output_en/test_results.txt!\n",
            "Computing prediction loss on test labels...\n",
            "Test loss: 0.6406\n"
          ]
        }
      ],
      "source": [
        "!python text_class.py \\\n",
        "--model_name_or_path google/muril-base-cased \\\n",
        "--train_file train_en.csv \\\n",
        "--validation_file validate_en.csv \\\n",
        "--output_dir output_en/ \\\n",
        "--test_file test_en.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python text_class.py \\\n",
        "--model_name_or_path google/muril-base-cased \\\n",
        "--train_file train_bn.csv \\\n",
        "--validation_file validate_bn.csv \\\n",
        "--output_dir output_bn/ \\\n",
        "--test_file test_bn.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZPL0i6QcZMn",
        "outputId": "152a23e5-97aa-48ef-bf7e-859139164a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/20/2022 09:18:13 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gcp_project=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_bn/runs/Jan20_09-18-13_56191b56896d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=output_bn/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "poly_power=1.0,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_bn/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_name=None,\n",
            "tpu_num_cores=None,\n",
            "tpu_zone=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xla=False,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/20/2022 09:18:13 - INFO - __main__ - Loading a local file for train: train_bn.csv\n",
            "01/20/2022 09:18:13 - INFO - __main__ - Loading a local file for validation: validate_bn.csv\n",
            "01/20/2022 09:18:13 - INFO - __main__ - Loading a local file for test: test_bn.csv\n",
            "01/20/2022 09:18:13 - WARNING - datasets.builder - Using custom data configuration default-60af225629b77c62\n",
            "01/20/2022 09:18:13 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-60af225629b77c62/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
            "100% 3/3 [00:00<00:00, 377.23it/s]\n",
            "100% 2/2 [00:00<00:00, 12.80ba/s]\n",
            "100% 1/1 [00:00<00:00, 17.71ba/s]\n",
            "100% 1/1 [00:00<00:00, 20.50ba/s]\n",
            "2022-01-20 09:18:19.844414: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier', 'bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-01-20 09:18:26.055753: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 1106\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:0\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Epoch 1/3\n",
            "138/138 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.6830 - f1_m: 0.0704 - precision_m: 0.0558 - recall_m: 0.12922022-01-20 09:19:25.978414: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 369\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:4\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "2022-01-20 09:19:32.223832: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "138/138 [==============================] - 71s 359ms/step - loss: 0.6138 - accuracy: 0.6830 - f1_m: 0.0704 - precision_m: 0.0558 - recall_m: 0.1292 - val_loss: 0.6414 - val_accuracy: 0.7019 - val_f1_m: 0.4052 - val_precision_m: 0.3020 - val_recall_m: 0.7216\n",
            "Epoch 2/3\n",
            "138/138 [==============================] - ETA: 0s - loss: 0.5303 - accuracy: 0.7781 - f1_m: 0.3371 - precision_m: 0.2758 - recall_m: 0.55802022-01-20 09:20:52.098529: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "138/138 [==============================] - 47s 340ms/step - loss: 0.5303 - accuracy: 0.7781 - f1_m: 0.3371 - precision_m: 0.2758 - recall_m: 0.5580 - val_loss: 0.5175 - val_accuracy: 0.7724 - val_f1_m: 0.4352 - val_precision_m: 0.3343 - val_recall_m: 0.7361\n",
            "Epoch 3/3\n",
            "138/138 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.7817 - f1_m: 0.4719 - precision_m: 0.3909 - recall_m: 0.78022022-01-20 09:21:38.214184: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "138/138 [==============================] - 46s 332ms/step - loss: 0.5352 - accuracy: 0.7817 - f1_m: 0.4719 - precision_m: 0.3909 - recall_m: 0.7802 - val_loss: 0.5534 - val_accuracy: 0.7534 - val_f1_m: 0.4765 - val_precision_m: 0.6482 - val_recall_m: 0.4036\n",
            "01/20/2022 09:21:42 - INFO - __main__ - Doing predictions on test dataset...\n",
            "2022-01-20 09:21:42.123839: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 369\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:7\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "01/20/2022 09:21:48 - INFO - __main__ - Wrote predictions to output_bn/test_results.txt!\n",
            "Computing prediction loss on test labels...\n",
            "Test loss: 0.5332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python text_class.py \\\n",
        "--model_name_or_path google/muril-base-cased \\\n",
        "--train_file train_hi.csv \\\n",
        "--validation_file validate_hi.csv \\\n",
        "--output_dir output_hi/ \\\n",
        "--test_file test_hi.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuvOJ1m0cZ-o",
        "outputId": "666938d3-37cc-43bc-f700-7f34c97faa25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/20/2022 09:21:56 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gcp_project=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_hi/runs/Jan20_09-21-56_56191b56896d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=output_hi/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "poly_power=1.0,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_hi/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_name=None,\n",
            "tpu_num_cores=None,\n",
            "tpu_zone=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xla=False,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/20/2022 09:21:56 - INFO - __main__ - Loading a local file for train: train_hi.csv\n",
            "01/20/2022 09:21:56 - INFO - __main__ - Loading a local file for validation: validate_hi.csv\n",
            "01/20/2022 09:21:56 - INFO - __main__ - Loading a local file for test: test_hi.csv\n",
            "01/20/2022 09:21:56 - WARNING - datasets.builder - Using custom data configuration default-c7c1fdd8927f71fa\n",
            "01/20/2022 09:21:56 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c7c1fdd8927f71fa/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
            "100% 3/3 [00:00<00:00, 416.05it/s]\n",
            "100% 2/2 [00:00<00:00, 14.58ba/s]\n",
            "100% 1/1 [00:00<00:00, 23.17ba/s]\n",
            "100% 1/1 [00:00<00:00, 24.41ba/s]\n",
            "2022-01-20 09:22:02.616850: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier', 'bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-01-20 09:22:05.484832: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 1046\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:0\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Epoch 1/3\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.5924 - accuracy: 0.7375 - f1_m: 0.1290 - precision_m: 0.1037 - recall_m: 0.20092022-01-20 09:23:02.759302: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 349\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:4\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "2022-01-20 09:23:08.582846: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "130/130 [==============================] - 67s 356ms/step - loss: 0.5924 - accuracy: 0.7375 - f1_m: 0.1290 - precision_m: 0.1037 - recall_m: 0.2009 - val_loss: 0.4653 - val_accuracy: 0.8223 - val_f1_m: 0.6229 - val_precision_m: 0.4807 - val_recall_m: 0.9606\n",
            "Epoch 2/3\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.8029 - f1_m: 0.5731 - precision_m: 0.4572 - recall_m: 0.86232022-01-20 09:23:50.930722: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "130/130 [==============================] - 42s 321ms/step - loss: 0.4807 - accuracy: 0.8029 - f1_m: 0.5731 - precision_m: 0.4572 - recall_m: 0.8623 - val_loss: 0.5117 - val_accuracy: 0.7880 - val_f1_m: 0.6295 - val_precision_m: 0.4879 - val_recall_m: 0.9795\n",
            "Epoch 3/3\n",
            "130/130 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.7587 - f1_m: 0.5853 - precision_m: 0.6338 - recall_m: 0.70722022-01-20 09:24:32.771004: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "130/130 [==============================] - 42s 325ms/step - loss: 0.5456 - accuracy: 0.7587 - f1_m: 0.5853 - precision_m: 0.6338 - recall_m: 0.7072 - val_loss: 0.6688 - val_accuracy: 0.6132 - val_f1_m: 0.2900 - val_precision_m: 0.5625 - val_recall_m: 0.2148\n",
            "01/20/2022 09:24:37 - INFO - __main__ - Doing predictions on test dataset...\n",
            "2022-01-20 09:24:37.293277: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 349\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:7\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "01/20/2022 09:24:43 - INFO - __main__ - Wrote predictions to output_hi/test_results.txt!\n",
            "Computing prediction loss on test labels...\n",
            "Test loss: 0.6530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python text_class.py \\\n",
        "--model_name_or_path google/muril-base-cased \\\n",
        "--train_file train_ml.csv \\\n",
        "--validation_file validate_ml.csv \\\n",
        "--output_dir output_ml/ \\\n",
        "--test_file test_ml.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsNybEa7cadI",
        "outputId": "3905e17d-dbd6-4e7b-dd61-4e706ff8f0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/20/2022 09:24:49 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gcp_project=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_ml/runs/Jan20_09-24-49_56191b56896d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=output_ml/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "poly_power=1.0,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_ml/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_name=None,\n",
            "tpu_num_cores=None,\n",
            "tpu_zone=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xla=False,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/20/2022 09:24:49 - INFO - __main__ - Loading a local file for train: train_ml.csv\n",
            "01/20/2022 09:24:49 - INFO - __main__ - Loading a local file for validation: validate_ml.csv\n",
            "01/20/2022 09:24:49 - INFO - __main__ - Loading a local file for test: test_ml.csv\n",
            "01/20/2022 09:24:49 - WARNING - datasets.builder - Using custom data configuration default-c178e9c76d831e3f\n",
            "01/20/2022 09:24:49 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c178e9c76d831e3f/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
            "100% 3/3 [00:00<00:00, 324.02it/s]\n",
            "100% 2/2 [00:00<00:00,  8.64ba/s]\n",
            "100% 1/1 [00:00<00:00, 11.75ba/s]\n",
            "100% 1/1 [00:00<00:00, 11.96ba/s]\n",
            "2022-01-20 09:24:55.526790: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier', 'bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-01-20 09:24:58.703441: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 1164\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:0\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Epoch 1/3\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.5768 - accuracy: 0.7216 - f1_m: 0.3177 - precision_m: 0.3365 - recall_m: 0.31872022-01-20 09:26:02.694658: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 388\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:4\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "2022-01-20 09:26:09.357278: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "145/145 [==============================] - 75s 369ms/step - loss: 0.5768 - accuracy: 0.7216 - f1_m: 0.3177 - precision_m: 0.3365 - recall_m: 0.3187 - val_loss: 0.4653 - val_accuracy: 0.7887 - val_f1_m: 0.8849 - val_precision_m: 0.8491 - val_recall_m: 0.9392\n",
            "Epoch 2/3\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.5538 - accuracy: 0.7440 - f1_m: 0.6782 - precision_m: 0.7199 - recall_m: 0.71502022-01-20 09:26:59.724246: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "145/145 [==============================] - 50s 344ms/step - loss: 0.5538 - accuracy: 0.7440 - f1_m: 0.6782 - precision_m: 0.7199 - recall_m: 0.7150 - val_loss: 0.5582 - val_accuracy: 0.7577 - val_f1_m: 0.8325 - val_precision_m: 0.7577 - val_recall_m: 0.9446\n",
            "Epoch 3/3\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.7060 - f1_m: 0.2554 - precision_m: 0.4135 - recall_m: 0.21782022-01-20 09:27:50.176616: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "145/145 [==============================] - 51s 349ms/step - loss: 0.5894 - accuracy: 0.7060 - f1_m: 0.2554 - precision_m: 0.4135 - recall_m: 0.2178 - val_loss: 0.5576 - val_accuracy: 0.7577 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "01/20/2022 09:28:25 - INFO - __main__ - Doing predictions on test dataset...\n",
            "2022-01-20 09:28:25.584923: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 388\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:7\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "01/20/2022 09:28:32 - INFO - __main__ - Wrote predictions to output_ml/test_results.txt!\n",
            "Computing prediction loss on test labels...\n",
            "Test loss: 0.5817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python text_class.py \\\n",
        "--model_name_or_path google/muril-base-cased \\\n",
        "--train_file train_ta.csv \\\n",
        "--validation_file validate_ta.csv \\\n",
        "--output_dir output_ta/ \\\n",
        "--test_file test_ta.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivv2TJrHca7Z",
        "outputId": "873a8a9c-e7f4-4ef4-9f63-602242ea6f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/20/2022 09:28:38 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gcp_project=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_ta/runs/Jan20_09-28-38_56191b56896d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=output_ta/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "poly_power=1.0,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_ta/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_name=None,\n",
            "tpu_num_cores=None,\n",
            "tpu_zone=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xla=False,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/20/2022 09:28:38 - INFO - __main__ - Loading a local file for train: train_ta.csv\n",
            "01/20/2022 09:28:38 - INFO - __main__ - Loading a local file for validation: validate_ta.csv\n",
            "01/20/2022 09:28:38 - INFO - __main__ - Loading a local file for test: test_ta.csv\n",
            "01/20/2022 09:28:38 - WARNING - datasets.builder - Using custom data configuration default-30f70ec013e750fb\n",
            "01/20/2022 09:28:38 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-30f70ec013e750fb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
            "100% 3/3 [00:00<00:00, 463.24it/s]\n",
            "100% 1/1 [00:00<00:00, 16.75ba/s]\n",
            "100% 1/1 [00:00<00:00, 47.54ba/s]\n",
            "100% 1/1 [00:00<00:00, 49.22ba/s]\n",
            "2022-01-20 09:28:44.084482: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier', 'bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-01-20 09:28:46.433232: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 396\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:0\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Epoch 1/3\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.6631 - accuracy: 0.6888 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+002022-01-20 09:29:21.433635: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 132\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:4\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "2022-01-20 09:29:25.566171: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "49/49 [==============================] - 43s 446ms/step - loss: 0.6631 - accuracy: 0.6888 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5740 - val_accuracy: 0.8561 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/3\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.6990 - f1_m: 0.1457 - precision_m: 0.2075 - recall_m: 0.12702022-01-20 09:29:44.743678: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "49/49 [==============================] - 19s 380ms/step - loss: 0.5771 - accuracy: 0.6990 - f1_m: 0.1457 - precision_m: 0.2075 - recall_m: 0.1270 - val_loss: 0.5224 - val_accuracy: 0.8485 - val_f1_m: 0.8559 - val_precision_m: 0.8539 - val_recall_m: 0.8804\n",
            "Epoch 3/3\n",
            "49/49 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8903 - f1_m: 0.7755 - precision_m: 0.6894 - recall_m: 0.95152022-01-20 09:30:03.371359: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 606059520 exceeds 10% of free system memory.\n",
            "49/49 [==============================] - 19s 386ms/step - loss: 0.3959 - accuracy: 0.8903 - f1_m: 0.7755 - precision_m: 0.6894 - recall_m: 0.9515 - val_loss: 0.3766 - val_accuracy: 0.8788 - val_f1_m: 0.7311 - val_precision_m: 0.6120 - val_recall_m: 0.9711\n",
            "01/20/2022 09:30:07 - INFO - __main__ - Doing predictions on test dataset...\n",
            "2022-01-20 09:30:07.498388: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "input: \"Placeholder/_3\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_VARIANT\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 132\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:7\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "01/20/2022 09:30:11 - INFO - __main__ - Wrote predictions to output_ta/test_results.txt!\n",
            "Computing prediction loss on test labels...\n",
            "Test loss: 0.2751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_en_res = pd.read_csv('output_en/test_results.txt', sep=\"\\t\")\n",
        "test_en = pd.read_csv('test_en.csv')\n",
        "test_en_res = test_en_res.prediction\n",
        "test_en.label\n",
        "\n",
        "print(\"EN: Classification report: \\n\", (classification_report(test_en.label, test_en_res)))\n",
        "print(\"EN: F1 micro averaging:\",(f1_score(test_en.label, test_en_res, average='micro')))\n",
        "print(\"EN: F1 macro averaging:\",(f1_score(test_en.label, test_en_res, average='macro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv-7VlmMhyGy",
        "outputId": "9cfea2ef-4944-4af7-fd19-4b184091370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.11      0.19        84\n",
            "           1       0.57      1.00      0.73       101\n",
            "\n",
            "    accuracy                           0.59       185\n",
            "   macro avg       0.79      0.55      0.46       185\n",
            "weighted avg       0.77      0.59      0.49       185\n",
            "\n",
            "EN: F1 micro averaging: 0.5945945945945946\n",
            "EN: F1 macro averaging: 0.46139513217654593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_hi_res = pd.read_csv('output_hi/test_results.txt', sep=\"\\t\")\n",
        "test_hi = pd.read_csv('test_hi.csv')\n",
        "test_hi_res = test_hi_res.prediction\n",
        "test_hi.label\n",
        "\n",
        "print(\"HI: Classification report: \\n\", (classification_report(test_hi.label, test_hi_res)))\n",
        "print(\"HI: F1 micro averaging:\",(f1_score(test_hi.label, test_hi_res, average='micro')))\n",
        "print(\"HI: F1 macro averaging:\",(f1_score(test_hi.label, test_hi_res, average='macro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjVmsl2QiN95",
        "outputId": "53cd5f3f-d426-4c00-b046-9e15c3dd6a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HI: Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.98      0.72       173\n",
            "           1       0.92      0.27      0.42       176\n",
            "\n",
            "    accuracy                           0.62       349\n",
            "   macro avg       0.75      0.62      0.57       349\n",
            "weighted avg       0.75      0.62      0.57       349\n",
            "\n",
            "HI: F1 micro averaging: 0.6217765042979942\n",
            "HI: F1 macro averaging: 0.57010078387458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ml_res = pd.read_csv('output_ml/test_results.txt', sep=\"\\t\")\n",
        "test_ml = pd.read_csv('test_ml.csv')\n",
        "test_ml_res = test_ml_res.prediction\n",
        "test_ml.label\n",
        "\n",
        "print(\"ML: Classification report: \\n\", (classification_report(test_ml.label, test_ml_res)))\n",
        "print(\"ML: F1 micro averaging:\",(f1_score(test_ml.label, test_ml_res, average='micro')))\n",
        "print(\"ML: F1 macro averaging:\",(f1_score(test_ml.label, test_ml_res, average='macro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOVnx7g1i4X7",
        "outputId": "4f5761a5-7aa1-4488-e1c4-8271fcd6e1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ML: Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       104\n",
            "           1       0.73      1.00      0.85       284\n",
            "\n",
            "    accuracy                           0.73       388\n",
            "   macro avg       0.37      0.50      0.42       388\n",
            "weighted avg       0.54      0.73      0.62       388\n",
            "\n",
            "ML: F1 micro averaging: 0.7319587628865979\n",
            "ML: F1 macro averaging: 0.42261904761904756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ta_res = pd.read_csv('output_ta/test_results.txt', sep=\"\\t\")\n",
        "test_ta = pd.read_csv('test_ta.csv')\n",
        "test_ta_res = test_ta_res.prediction\n",
        "test_ta.label\n",
        "\n",
        "print(\"TA: Classification report: \\n\", (classification_report(test_ta.label, test_ta_res)))\n",
        "print(\"TA: F1 micro averaging:\",(f1_score(test_ta.label, test_ta_res, average='micro')))\n",
        "print(\"TA: F1 macro averaging:\",(f1_score(test_ta.label, test_ta_res, average='macro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6_dncn5jIUR",
        "outputId": "22ce2a12-1c32-4b7a-82fc-3f058d22497c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TA: Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87        45\n",
            "           1       0.93      0.93      0.93        87\n",
            "\n",
            "    accuracy                           0.91       132\n",
            "   macro avg       0.90      0.90      0.90       132\n",
            "weighted avg       0.91      0.91      0.91       132\n",
            "\n",
            "TA: F1 micro averaging: 0.9090909090909091\n",
            "TA: F1 macro averaging: 0.8988505747126436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_bn_res = pd.read_csv('output_bn/test_results.txt', sep=\"\\t\")\n",
        "test_bn = pd.read_csv('test_bn.csv')\n",
        "test_bn_res = test_bn_res.prediction\n",
        "test_bn.label\n",
        "\n",
        "print(\"BN: Classification report: \\n\", (classification_report(test_bn.label, test_bn_res)))\n",
        "print(\"BN: F1 micro averaging:\",(f1_score(test_bn.label, test_bn_res, average='micro')))\n",
        "print(\"BN: F1 macro averaging:\",(f1_score(test_bn.label, test_bn_res, average='macro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXnPAjNzjfWC",
        "outputId": "e93181b1-fba0-474f-ecfa-e23ad9ca8f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BN: Classification report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.96      0.85       241\n",
            "           1       0.86      0.42      0.57       128\n",
            "\n",
            "    accuracy                           0.78       369\n",
            "   macro avg       0.81      0.69      0.71       369\n",
            "weighted avg       0.79      0.78      0.75       369\n",
            "\n",
            "BN: F1 micro averaging: 0.7750677506775068\n",
            "BN: F1 macro averaging: 0.7068541401456779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsvvZJXN5JmV",
        "outputId": "4c05cce2-ec8d-4903-827f-2bd1d2ab3a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('/content/drive/MyDrive/Multi_XLM/output_en')\n",
        "os.makedirs('/content/drive/MyDrive/Multi_XLM/output_hi')\n",
        "os.makedirs('/content/drive/MyDrive/Multi_XLM/output_ml')\n",
        "os.makedirs('/content/drive/MyDrive/Multi_XLM/output_bn')\n",
        "os.makedirs('/content/drive/MyDrive/Multi_XLM/output_ta')"
      ],
      "metadata": {
        "id": "XSn-nM3F5NYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp output_en/* /content/drive/MyDrive/Multi_XLM/output_en\n",
        "!cp output_hi/* /content/drive/MyDrive/Multi_XLM/output_hi\n",
        "!cp output_ta/* /content/drive/MyDrive/Multi_XLM/output_ta\n",
        "!cp output_ml/* /content/drive/MyDrive/Multi_XLM/output_ml\n",
        "!cp output_bn/* /content/drive/MyDrive/Multi_XLM/output_bn"
      ],
      "metadata": {
        "id": "W2wZcsAs5QUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp * /content/drive/MyDrive/Multi_XLM/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOuXi9kI5Sws",
        "outputId": "b1c2d8a9-c5b4-4fc2-86d4-7e88c9930dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: -r not specified; omitting directory 'drive'\n",
            "cp: -r not specified; omitting directory 'output_bn'\n",
            "cp: -r not specified; omitting directory 'output_en'\n",
            "cp: -r not specified; omitting directory 'output_hi'\n",
            "cp: -r not specified; omitting directory 'output_ml'\n",
            "cp: -r not specified; omitting directory 'output_ta'\n",
            "cp: -r not specified; omitting directory 'sample_data'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Approach2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}